{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LoraQween: Colab Training\n",
        "\n",
        "This notebook mounts Google Drive, sets up the project, installs deps, audits the dataset, and launches training for a person (e.g., Mikassa).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch, platform\n",
        "print('Torch:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU name:', torch.cuda.get_device_name(0))\n",
        "print('Python:', platform.python_version())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_DIR_DRIVE = '/content/drive/MyDrive/LoraQween'\n",
        "PROJECT_DIR = '/content/LoraQween'\n",
        "\n",
        "print('Drive project:', PROJECT_DIR_DRIVE)\n",
        "print('Local project:', PROJECT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy project (code and dataset) from Drive to fast local storage\n",
        "mkdir -p /content/LoraQween\n",
        "\n",
        "# Core project\n",
        "cp -r \"$PROJECT_DIR_DRIVE/config.yaml\" /content/LoraQween/ 2>/dev/null || true\n",
        "cp -r \"$PROJECT_DIR_DRIVE/scripts\" /content/LoraQween/ 2>/dev/null || true\n",
        "cp -r \"$PROJECT_DIR_DRIVE/src\" /content/LoraQween/ 2>/dev/null || true\n",
        "\n",
        "# Dataset (person folder)\n",
        "mkdir -p /content/LoraQween/data/datasets\n",
        "cp -r \"$PROJECT_DIR_DRIVE/data/datasets/Mikassa\" /content/LoraQween/data/datasets/ 2>/dev/null || true\n",
        "\n",
        "# Optional docs\n",
        "cp -r \"$PROJECT_DIR_DRIVE/docs\" /content/LoraQween/ 2>/dev/null || true\n",
        "\n",
        "ls -la /content/LoraQween\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (excluding torch to keep Colab's preinstalled CUDA build)\n",
        "import sys, subprocess\n",
        "\n",
        "def pip_install(args):\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\"] + args\n",
        "    print(\"Installing:\", \" \".join(args))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "pip_install([\"pip\", \"setuptools\", \"wheel\"])\n",
        "pip_install([\n",
        "    \"transformers>=4.40.0\",\n",
        "    \"diffusers>=0.29.0\",\n",
        "    \"accelerate>=0.29.0\",\n",
        "    \"peft>=0.10.0\",\n",
        "    \"Pillow\",\n",
        "    \"opencv-python-headless\",\n",
        "    \"numpy\",\n",
        "    \"pandas\",\n",
        "    \"pyyaml\",\n",
        "    \"tqdm\",\n",
        "    \"psutil\",\n",
        "    \"tensorboard\",\n",
        "    \"exifread\",\n",
        "    \"piexif\",\n",
        "    \"pynvml\",\n",
        "])\n",
        "\n",
        "import torch\n",
        "print('Torch:', torch.__version__, '| CUDA available:', torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Env: set Hugging Face cache locally (fast), and persist outputs/loras to Drive via symlinks\n",
        "import os, pathlib, subprocess\n",
        "\n",
        "HF_CACHE = \"/content/hf_cache\"\n",
        "pathlib.Path(HF_CACHE).mkdir(parents=True, exist_ok=True)\n",
        "os.environ[\"HF_HOME\"] = HF_CACHE\n",
        "os.environ[\"HF_HUB_CACHE\"] = str(pathlib.Path(HF_CACHE) / \"hub\")\n",
        "print('HF_HOME =', os.environ[\"HF_HOME\"])  \n",
        "print('HF_HUB_CACHE =', os.environ[\"HF_HUB_CACHE\"])  \n",
        "\n",
        "# Link outputs and loras to Drive (to persist results)\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/LoraQween\"\n",
        "LOCAL_BASE = \"/content/LoraQween\"\n",
        "\n",
        "for rel in [\"models/loras\", \"outputs\", \"logs\"]:\n",
        "    drive_path = pathlib.Path(DRIVE_BASE) / rel\n",
        "    local_path = pathlib.Path(LOCAL_BASE) / rel\n",
        "    drive_path.mkdir(parents=True, exist_ok=True)\n",
        "    if local_path.exists() or local_path.is_symlink():\n",
        "        subprocess.run([\"rm\", \"-rf\", str(local_path)], check=False)\n",
        "    subprocess.run([\"ln\", \"-s\", str(drive_path), str(local_path)], check=False)\n",
        "    print(\"Linked:\", local_path, \"->\", drive_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Audit dataset to ensure integrity\n",
        "import subprocess, os\n",
        "os.chdir('/content/LoraQween')\n",
        "subprocess.check_call([\"python\", \"scripts/audit_dataset.py\", \"--person\", \"Mikassa\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch training\n",
        "import subprocess, os\n",
        "os.chdir('/content/LoraQween')\n",
        "subprocess.check_call([\"python\", \"scripts/train_lora.py\", \"--person\", \"Mikassa\"])\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
